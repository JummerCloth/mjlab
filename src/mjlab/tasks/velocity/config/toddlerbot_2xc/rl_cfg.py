"""RL configuration for ToddlerBot velocity task."""

from mjlab.rl import (
  RslRlOnPolicyRunnerCfg,
  RslRlPpoActorCriticCfg,
  RslRlPpoAlgorithmCfg,
)


def toddlerbot_ppo_runner_cfg() -> RslRlOnPolicyRunnerCfg:
  """Create RL runner configuration for ToddlerBot velocity task.

  Uses similar hyperparameters to G1 for the mjlab velocity tracking task.
  The original ToddlerBot codebase used custom observation scaling, but since
  we're using standard observations with running normalization, we use
  standard hyperparameters.
  """
  return RslRlOnPolicyRunnerCfg(
    policy=RslRlPpoActorCriticCfg(
      init_noise_std=1.0,
      actor_obs_normalization=True,  # Use running normalization
      critic_obs_normalization=True,  # Use running normalization
      actor_hidden_dims=(512, 256, 128),
      critic_hidden_dims=(512, 256, 128),
      activation="elu",
    ),
    algorithm=RslRlPpoAlgorithmCfg(
      value_loss_coef=1.0,
      use_clipped_value_loss=True,
      clip_param=0.2,
      entropy_coef=0.01,  # Standard exploration coefficient
      num_learning_epochs=5,
      num_mini_batches=4,
      learning_rate=1.0e-3,  # Standard learning rate
      schedule="adaptive",
      gamma=0.99,
      lam=0.95,
      desired_kl=0.01,
      max_grad_norm=1.0,
    ),
    experiment_name="toddlerbot_velocity",
    save_interval=50,
    num_steps_per_env=24,
    max_iterations=30_000,
  )
